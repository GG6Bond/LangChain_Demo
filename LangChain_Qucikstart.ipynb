{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abce623f",
   "metadata": {},
   "source": [
    "### LLM_Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9d3c8c",
   "metadata": {},
   "source": [
    "使用本地的开源模型，利用ollama来运行llama2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcb7d2a",
   "metadata": {},
   "source": [
    "确保ollama服务处于运行状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad88f2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model=\"llama2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03f7b36",
   "metadata": {},
   "source": [
    "接下来我们就可以尝试使用llm调用llama2模型了。我们问它\"LangSmith\"是什么。\n",
    "这是训练数据中没有出现过的东西，所有它并不能给出很好的回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633e1ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(\"how can langsmith help with testing?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c28625",
   "metadata": {},
   "source": [
    "我们也可以使用提示模板来指导模型作出响应。提示模版的作用是，将用户的原始输入转换为对于llm更好的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5ad375",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are world class technical documentation writer.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056c02a9",
   "metadata": {},
   "source": [
    "现在我们可以将这些组合成一个简单的LLM链"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24d2964",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e666136",
   "metadata": {},
   "source": [
    "我们现在可以调用它并问同样的问题。虽然它仍然不知道答案，但能以一种更适合技术作家的语气来回应!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5975d064",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b886f1b2",
   "metadata": {},
   "source": [
    "ChatModel的输出(也是这个链的输出)是一条消息。然而，使用字符串通常要方便得多。让我们添加一个简单的输出解析器，将聊天消息转换为字符串。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5718216f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d4c0d6",
   "metadata": {},
   "source": [
    "现在可以把这个添加到前面的链中:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5666bab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1807ee",
   "metadata": {},
   "source": [
    "现在可以调用它并问同样的问题。现在答案将是一个字符串(而不是ChatMessage)。·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25eef16",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a568b0b",
   "metadata": {},
   "source": [
    "### Retrieval_Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5162f30c",
   "metadata": {},
   "source": [
    "为了正确回答最初的问题(“langsmith如何帮助测试?”)，我们需要为LLM提供额外的知识。可以通过检索来实现。当有太多的数据需要直接传递给LLM时，检索非常有用。然后，可以使用检索器只获取最相关的部分并将它们传递给LLM。在这个过程中，我们将查找相关文档，然后将它们传递给提示词。检索器可以由任何东西支持，比如SQL表、互联网等。但在本例中，我们将填充一个向量存储并将其用作检索器。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec308c07",
   "metadata": {},
   "source": [
    "首先，我们需要加载要建立索引的数据。为此，我们将使用WebBaseLoader。这需要安装BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e7d3de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44b3920",
   "metadata": {},
   "source": [
    "之后，我们可以导入和使用WebBaseLoader。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae0c143",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/user_guide\")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf67989",
   "metadata": {},
   "source": [
    "接下来，我们需要将其索引到vectorstore中。这需要几个组件，即embedding模型和vectorstore。\n",
    "\n",
    "对于嵌入模型，我们再次提供了通过API或运行本地模型进行访问的示例。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14109f68",
   "metadata": {},
   "source": [
    "确保Ollama正在运行(与LLM的设置相同)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ab199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e254ed1",
   "metadata": {},
   "source": [
    "现在，我们可以使用这个嵌入模型将文档存储到vectorstore中。为了简单起见，我们将使用一个简单的本地矢量库FAISS。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203ec909",
   "metadata": {},
   "source": [
    "首先，我们需要安装所需的软件包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f641229d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4134914b",
   "metadata": {},
   "source": [
    "然后我们可以建立索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6baeb0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62c6f90",
   "metadata": {},
   "source": [
    "现在我们已经在vectorstore中为这些数据建立了索引，我们将创建一个检索链。该链将接收传入的问题，查找相关文档，然后将这些文档与原始问题一起传递给LLM，并要求它回答原始问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936d121f",
   "metadata": {},
   "source": [
    "首先，让我们建立一个链，它接受一个问题和检索到的文档，并生成一个答案。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7371cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1069b2c9",
   "metadata": {},
   "source": [
    "如果需要，我们可以通过直接传递文档来运行这个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a81eef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "document_chain.invoke({\n",
    "    \"input\": \"how can langsmith help with testing?\",\n",
    "    \"context\": [Document(page_content=\"langsmith can let you visualize test results\")]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58786a40",
   "metadata": {},
   "source": [
    "然而，我们希望文档首先来自我们刚刚设置的检索器。这样，我们可以使用检索器动态地选择最相关的文档，并为给定的问题传递这些文档。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9f1e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962c0595",
   "metadata": {},
   "source": [
    "现在我们可以调用这个链。这将返回一个字典，来自LLM的响应在答案键中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cd0b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "print(response[\"answer\"])\n",
    "\n",
    "# LangSmith offers several features that can help with testing:..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8177b020",
   "metadata": {},
   "source": [
    "这个答案应该准确得多"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82095ce",
   "metadata": {},
   "source": [
    "### Conversation Retrieval Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167f4250",
   "metadata": {},
   "source": [
    "The chain we've created so far can only answer single questions. One of the main types of LLM applications that people are building are chat bots. So how do we turn this chain into one that can answer follow up questions?\n",
    "\n",
    "We can still use the create_retrieval_chain function, but we need to change two things:\n",
    "\n",
    "1.The retrieval method should now not just work on the most recent input, but rather should take the whole history into account.\n",
    "\n",
    "2.The final LLM chain should likewise take the whole history into account"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf854c2",
   "metadata": {},
   "source": [
    "#### Updating Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc9ab2b",
   "metadata": {},
   "source": [
    "In order to update retrieval, we will create a new chain. This chain will take in the most recent input (input) and the conversation history (chat_history) and use an LLM to generate a search query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068ddaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "# First we need a prompt that we can pass into an LLM to generate this search query\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"user\", \"Given the above conversation, generate a search query to look up to get information relevant to the conversation\")\n",
    "])\n",
    "retriever_chain = create_history_aware_retriever(llm, retriever, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5c26c2",
   "metadata": {},
   "source": [
    "We can test this out by passing in an instance where the user asks a follow-up question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37896227",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
    "retriever_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd91b580",
   "metadata": {},
   "source": [
    "You should see that this returns documents about testing in LangSmith. This is because the LLM generated a new query, combining the chat history with the follow-up question.\n",
    "\n",
    "Now that we have this new retriever, we can create a new chain to continue the conversation with these retrieved documents in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46547d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the user's questions based on the below context:\\n\\n{context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cc054d",
   "metadata": {},
   "source": [
    "We can now test this out end-to-end:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0264c040",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
    "retrieval_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ea3190",
   "metadata": {},
   "source": [
    "We can see that this gives a coherent answer - we've successfully turned our retrieval chain into a chatbot!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514cdb1f",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac48d41",
   "metadata": {},
   "source": [
    "We've so far created examples of chains - where each step is known ahead of time. The final thing we will create is an agent - where the LLM decides what steps to take."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eb9924",
   "metadata": {},
   "source": [
    "**NOTE: for this example we will only show how to create an agent using OpenAI models, as local models are not reliable enough yet.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12984141",
   "metadata": {},
   "source": [
    "One of the first things to do when building an agent is to decide what tools it should have access to. For this example, we will give the agent access to two tools:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2f00eb",
   "metadata": {},
   "source": [
    "1.The retriever we just created. This will let it easily answer questions about LangSmith\n",
    "\n",
    "2.A search tool. This will let it easily answer questions that require up-to-date information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061c97b6",
   "metadata": {},
   "source": [
    "First, let's set up a tool for the retriever we just created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c62695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"langsmith_search\",\n",
    "    \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65fae00",
   "metadata": {},
   "source": [
    "The search tool that we will use is Tavily. This will require an API key (they have generous free tier). After creating it on their platform, you need to set it as an environment variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ebe5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "export TAVILY_API_KEY=..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372d474c",
   "metadata": {},
   "source": [
    "If you do not want to set up an API key, you can skip creating this tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76b84ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "search = TavilySearchResults()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7215283",
   "metadata": {},
   "source": [
    "We can now create a list of the tools we want to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e05f57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [retriever_tool, search]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb364edd",
   "metadata": {},
   "source": [
    "Now that we have the tools, we can create an agent to use them. We will go over this pretty quickly - for a deeper dive into what exactly is going on, check out the Agent's Getting Started documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3465a4",
   "metadata": {},
   "source": [
    "Install langchain hub first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e8fe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchainhub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fb3934",
   "metadata": {},
   "source": [
    "Install the langchain-openai package To interact with OpenAI we need to use langchain-openai which connects with OpenAI SDK[https://github.com/langchain-ai/langchain/tree/master/libs/partners/openai]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7152520c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f79a65",
   "metadata": {},
   "source": [
    "Now we can use it to get a predefined prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ecee52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "from langchain.agents import create_openai_functions_agent\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "\n",
    "# You need to set OPENAI_API_KEY environment variable or pass it as argument `openai_api_key`.\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "agent = create_openai_functions_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3274d8f9",
   "metadata": {},
   "source": [
    "We can now invoke the agent and see how it responds! We can ask it questions about LangSmith:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c6d23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3f9511",
   "metadata": {},
   "source": [
    "We can ask it about the weather:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3ba572",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": \"what is the weather in SF?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a32237a",
   "metadata": {},
   "source": [
    "We can have conversations with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8199a263",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
    "agent_executor.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c6d585",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
