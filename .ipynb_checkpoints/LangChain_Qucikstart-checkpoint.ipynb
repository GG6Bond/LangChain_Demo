{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04852fef",
   "metadata": {},
   "source": [
    "### LLM_Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b3805c",
   "metadata": {},
   "source": [
    "使用本地的开源模型，利用ollama来运行llama2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a641b7dc",
   "metadata": {},
   "source": [
    "确保ollama服务处于运行状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14b3659",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model=\"llama2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc09dc32",
   "metadata": {},
   "source": [
    "接下来我们就可以尝试使用llm调用llama2模型了。我们问它\"LangSmith\"是什么。\n",
    "这是训练数据中没有出现过的东西，所有它并不能给出很好的回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c807f94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(\"how can langsmith help with testing?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d763fed",
   "metadata": {},
   "source": [
    "我们也可以使用提示模板来指导模型作出响应。提示模版的作用是，将用户的原始输入转换为对于llm更好的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd7b16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are world class technical documentation writer.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f584eb",
   "metadata": {},
   "source": [
    "现在我们可以将这些组合成一个简单的LLM链"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c1693a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe3aa78",
   "metadata": {},
   "source": [
    "我们现在可以调用它并问同样的问题。虽然它仍然不知道答案，但能以一种更适合技术作家的语气来回应!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd5ba28",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b94b8b",
   "metadata": {},
   "source": [
    "ChatModel的输出(也是这个链的输出)是一条消息。然而，使用字符串通常要方便得多。让我们添加一个简单的输出解析器，将聊天消息转换为字符串。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a104d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feef50f0",
   "metadata": {},
   "source": [
    "现在可以把这个添加到前面的链中:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60920dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23c624e",
   "metadata": {},
   "source": [
    "现在可以调用它并问同样的问题。现在答案将是一个字符串(而不是ChatMessage)。·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fc6c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e4737e",
   "metadata": {},
   "source": [
    "### Retrieval_Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233903db",
   "metadata": {},
   "source": [
    "为了正确回答最初的问题(“langsmith如何帮助测试?”)，我们需要为LLM提供额外的知识。可以通过检索来实现。当有太多的数据需要直接传递给LLM时，检索非常有用。然后，可以使用检索器只获取最相关的部分并将它们传递给LLM。在这个过程中，我们将查找相关文档，然后将它们传递给提示词。检索器可以由任何东西支持，比如SQL表、互联网等。但在本例中，我们将填充一个向量存储并将其用作检索器。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f42d783",
   "metadata": {},
   "source": [
    "首先，我们需要加载要建立索引的数据。为此，我们将使用WebBaseLoader。这需要安装BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f947c5e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802684d7",
   "metadata": {},
   "source": [
    "之后，我们可以导入和使用WebBaseLoader。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1615714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/user_guide\")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79af5584",
   "metadata": {},
   "source": [
    "接下来，我们需要将其索引到vectorstore中。这需要几个组件，即embedding模型和vectorstore。\n",
    "\n",
    "对于嵌入模型，我们再次提供了通过API或运行本地模型进行访问的示例。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8927c6c1",
   "metadata": {},
   "source": [
    "确保Ollama正在运行(与LLM的设置相同)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d31527",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66884ab",
   "metadata": {},
   "source": [
    "现在，我们可以使用这个嵌入模型将文档存储到vectorstore中。为了简单起见，我们将使用一个简单的本地矢量库FAISS。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5217862f",
   "metadata": {},
   "source": [
    "首先，我们需要安装所需的软件包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bca51be",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cc1395",
   "metadata": {},
   "source": [
    "然后我们可以建立索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191cca76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001c1a4f",
   "metadata": {},
   "source": [
    "现在我们已经在vectorstore中为这些数据建立了索引，我们将创建一个检索链。该链将接收传入的问题，查找相关文档，然后将这些文档与原始问题一起传递给LLM，并要求它回答原始问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1383080",
   "metadata": {},
   "source": [
    "首先，让我们建立一个链，它接受一个问题和检索到的文档，并生成一个答案。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5974fc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c157562",
   "metadata": {},
   "source": [
    "如果需要，我们可以通过直接传递文档来运行这个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1df5652",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "document_chain.invoke({\n",
    "    \"input\": \"how can langsmith help with testing?\",\n",
    "    \"context\": [Document(page_content=\"langsmith can let you visualize test results\")]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fd0a0e",
   "metadata": {},
   "source": [
    "然而，我们希望文档首先来自我们刚刚设置的检索器。这样，我们可以使用检索器动态地选择最相关的文档，并为给定的问题传递这些文档。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8242e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1f65ed",
   "metadata": {},
   "source": [
    "现在我们可以调用这个链。这将返回一个字典，来自LLM的响应在答案键中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe59f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "print(response[\"answer\"])\n",
    "\n",
    "# LangSmith offers several features that can help with testing:..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187d5c82",
   "metadata": {},
   "source": [
    "这个答案应该准确得多"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1436bdf1",
   "metadata": {},
   "source": [
    "### Conversation Retrieval Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f70a345",
   "metadata": {},
   "source": [
    "我们目前创建的链只能回答单一的问题。人们正在构建的LLM应用程序的主要类型之一是聊天机器人。那么，我们如何把这条链变成一条能够回答后续问题的链呢?\n",
    "\n",
    "我们仍然可以使用create_retrieval_chain函数，但是我们需要改变两件事:\n",
    "\n",
    "1.检索方法现在不应该只处理最近的输入，而是应该考虑整个历史。\n",
    "\n",
    "2.最后的LLM链也应该考虑到整个历史"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7b9ac0",
   "metadata": {},
   "source": [
    "**Updating Retrieval**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b784737",
   "metadata": {},
   "source": [
    "为了更新检索，我们将创建一个新的链。该链将接受最近的输入(input)和对话历史(chat_history)，并使用LLM生成搜索查询。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5b2c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "# First we need a prompt that we can pass into an LLM to generate this search query\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"user\", \"Given the above conversation, generate a search query to look up to get information relevant to the conversation\")\n",
    "])\n",
    "retriever_chain = create_history_aware_retriever(llm, retriever, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb9c2fd",
   "metadata": {},
   "source": [
    "我们可以通过传递一个用户提出后续问题的实例来对此进行测试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f258b4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
    "retriever_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7343a4",
   "metadata": {},
   "source": [
    "您应该看到，这将返回关于在LangSmith中进行测试的文档。这是因为LLM生成了一个新的查询，将聊天历史记录与后续问题结合起来。\n",
    "\n",
    "现在我们有了这个新的检索器，我们可以创建一个新的链，以便在记住这些检索到的文档的情况下继续对话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72e1fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the user's questions based on the below context:\\n\\n{context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66a9276",
   "metadata": {},
   "source": [
    "我们现在可以端到端测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5708494",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
    "retrieval_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b02b258",
   "metadata": {},
   "source": [
    "我们可以看到，这给出了一个连贯的答案——我们已经成功地把检索链变成了一个聊天机器人!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1178be",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bb2e73",
   "metadata": {},
   "source": [
    "到目前为止，我们已经创建了链的例子，其中每一步都是提前知道的。我们将创建的最后一个东西是代理——LLM决定采取什么步骤。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b8be92",
   "metadata": {},
   "source": [
    "**NOTE: 对于本例，我们将只展示如何使用OpenAI模型创建代理，因为本地模型还不够可靠。**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3e1275",
   "metadata": {},
   "source": [
    "构建代理时要做的第一件事是决定它应该访问哪些工具。对于本例，我们将为代理提供两个工具的访问权限:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21460cfa",
   "metadata": {},
   "source": [
    "1.我们刚刚创造的检索器。这将使它很容易回答有关LangSmith的问题\n",
    "\n",
    "2.一个搜索工具。这将使它很容易回答需要最新信息的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74287ebc",
   "metadata": {},
   "source": [
    "首先，让我们为刚刚创建的检索器设置一个工具:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bdf9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"langsmith_search\",\n",
    "    \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae16f1b9",
   "metadata": {},
   "source": [
    "我们将使用的搜索工具是Tavily。这将需要一个API密钥(他们有免费的)。在他们的平台上创建它之后，你需要将它设置为一个环境变量:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db642c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "export TAVILY_API_KEY=..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec60f93",
   "metadata": {},
   "source": [
    "如果您不想设置API密钥，可以跳过创建此工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0d9e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "search = TavilySearchResults()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0666e132",
   "metadata": {},
   "source": [
    "现在我们可以创建一个我们想要使用的工具列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319e2ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [retriever_tool, search]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb42df84",
   "metadata": {},
   "source": [
    "现在我们有了这些工具，我们可以创建一个代理来使用它们。我们将很快地过一遍这一点——要更深入地了解到底发生了什么，请查看Agent的入门文档"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92585e4c",
   "metadata": {},
   "source": [
    "Install langchain hub first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4046870",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchainhub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cad85e",
   "metadata": {},
   "source": [
    "要与OpenAI交互，我们需要使用与OpenAI SDK连接的langchain-openai [https://github.com/langchain-ai/langchain/tree/master/libs/partners/openai]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50833025",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884c0e3f",
   "metadata": {},
   "source": [
    "现在我们可以使用它来获得一个预定义的提示符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e79738",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "from langchain.agents import create_openai_functions_agent\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "\n",
    "# You need to set OPENAI_API_KEY environment variable or pass it as argument `openai_api_key`.\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "agent = create_openai_functions_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af59fae",
   "metadata": {},
   "source": [
    "现在我们可以调用代理，看看它是如何响应的!我们可以问它关于LangSmith的问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da210c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13f2017",
   "metadata": {},
   "source": [
    "我们可以问它关于天气的问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfdaeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": \"what is the weather in SF?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d9f03d",
   "metadata": {},
   "source": [
    "我们可以和它对话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c143b12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
    "agent_executor.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf047563",
   "metadata": {},
   "source": [
    "### Serving with LangServe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5d1ce4",
   "metadata": {},
   "source": [
    "现在我们已经构建了一个应用程序，我们需要为它提供服务。这就是LangServe的用武之地。LangServe帮助开发人员将LangChain链部署为REST API。你不需要使用LangServe来使用LangChain，但在本指南中，我们将展示如何使用LangServe部署应用程序。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ad7648",
   "metadata": {},
   "source": [
    "虽然本指南的第一部分在Jupyter Notebook中运行，但我们现在将不再使用它。我们将创建一个Python文件，然后从命令行与它交互。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d609de7",
   "metadata": {},
   "source": [
    "要为我们的应用程序创建一个服务器，我们将创建一个server .py文件。这将包含为应用程序提供服务的逻辑。它由三部分组成:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9c1d63",
   "metadata": {},
   "source": [
    "* 我们刚刚构建的链的定义\n",
    "* FastAPI应用\n",
    "* 为链提供服务的路由的定义，使用langserve.add_routes完成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fdbbee",
   "metadata": {},
   "source": [
    "```\n",
    "#!/usr/bin/env python\n",
    "from typing import List\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain import hub\n",
    "from langchain.agents import create_openai_functions_agent\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langserve import add_routes\n",
    "\n",
    "# 1. Load Retriever\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/user_guide\")\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vector = FAISS.from_documents(documents, embeddings)\n",
    "retriever = vector.as_retriever()\n",
    "\n",
    "# 2. Create Tools\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"langsmith_search\",\n",
    "    \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\",\n",
    ")\n",
    "search = TavilySearchResults()\n",
    "tools = [retriever_tool, search]\n",
    "\n",
    "\n",
    "# 3. Create Agent\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "agent = create_openai_functions_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "\n",
    "# 4. App definition\n",
    "app = FastAPI(\n",
    "  title=\"LangChain Server\",\n",
    "  version=\"1.0\",\n",
    "  description=\"A simple API server using LangChain's Runnable interfaces\",\n",
    ")\n",
    "\n",
    "# 5. Adding chain route\n",
    "\n",
    "# We need to add these input/output schemas because the current AgentExecutor\n",
    "# is lacking in schemas.\n",
    "\n",
    "class Input(BaseModel):\n",
    "    input: str\n",
    "    chat_history: List[BaseMessage] = Field(\n",
    "        ...,\n",
    "        extra={\"widget\": {\"type\": \"chat\", \"input\": \"location\"}},\n",
    "    )\n",
    "\n",
    "\n",
    "class Output(BaseModel):\n",
    "    output: str\n",
    "\n",
    "add_routes(\n",
    "    app,\n",
    "    agent_executor.with_types(input_type=Input, output_type=Output),\n",
    "    path=\"/agent\",\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "\n",
    "    uvicorn.run(app, host=\"localhost\", port=8000)\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0dfbe3",
   "metadata": {},
   "source": [
    "执行这个文件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04bb81d",
   "metadata": {},
   "source": [
    "```python serve.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8fafc2",
   "metadata": {},
   "source": [
    "我们可以看到我们的链在8000端口服务"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
