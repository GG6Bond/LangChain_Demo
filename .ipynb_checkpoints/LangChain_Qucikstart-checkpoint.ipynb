{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c9ad807",
   "metadata": {},
   "source": [
    "### LLM_Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4be58b",
   "metadata": {},
   "source": [
    "使用本地的开源模型，利用ollama来运行llama2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c05dc10",
   "metadata": {},
   "source": [
    "确保ollama服务处于运行状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e194e359",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model=\"llama2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15b1534",
   "metadata": {},
   "source": [
    "接下来我们就可以尝试使用llm调用llama2模型了。我们问它\"LangSmith\"是什么。\n",
    "这是训练数据中没有出现过的东西，所有它并不能给出很好的回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320a16d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(\"how can langsmith help with testing?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb65a8af",
   "metadata": {},
   "source": [
    "我们也可以使用提示模板来指导模型作出响应。提示模版的作用是，将用户的原始输入转换为对于llm更好的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a943f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are world class technical documentation writer.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445108f4",
   "metadata": {},
   "source": [
    "现在我们可以将这些组合成一个简单的LLM链"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdd05da",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8557ada",
   "metadata": {},
   "source": [
    "我们现在可以调用它并问同样的问题。虽然它仍然不知道答案，但能以一种更适合技术作家的语气来回应!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8cb375",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdde9552",
   "metadata": {},
   "source": [
    "ChatModel的输出(也是这个链的输出)是一条消息。然而，使用字符串通常要方便得多。让我们添加一个简单的输出解析器，将聊天消息转换为字符串。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8722d953",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472e45ec",
   "metadata": {},
   "source": [
    "现在可以把这个添加到前面的链中:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8f7bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87397f7",
   "metadata": {},
   "source": [
    "现在可以调用它并问同样的问题。现在答案将是一个字符串(而不是ChatMessage)。·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30183f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93483b01",
   "metadata": {},
   "source": [
    "### Retrieval_Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3910d090",
   "metadata": {},
   "source": [
    "为了正确回答最初的问题(“langsmith如何帮助测试?”)，我们需要为LLM提供额外的知识。可以通过检索来实现。当有太多的数据需要直接传递给LLM时，检索非常有用。然后，可以使用检索器只获取最相关的部分并将它们传递给LLM。在这个过程中，我们将查找相关文档，然后将它们传递给提示词。检索器可以由任何东西支持，比如SQL表、互联网等。但在本例中，我们将填充一个向量存储并将其用作检索器。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8893582",
   "metadata": {},
   "source": [
    "首先，我们需要加载要建立索引的数据。为此，我们将使用WebBaseLoader。这需要安装BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59189098",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b5919a",
   "metadata": {},
   "source": [
    "之后，我们可以导入和使用WebBaseLoader。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549eb3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/user_guide\")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c848ef4",
   "metadata": {},
   "source": [
    "接下来，我们需要将其索引到vectorstore中。这需要几个组件，即embedding模型和vectorstore。\n",
    "\n",
    "对于嵌入模型，我们再次提供了通过API或运行本地模型进行访问的示例。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3874c119",
   "metadata": {},
   "source": [
    "确保Ollama正在运行(与LLM的设置相同)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1d1c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32a9d54",
   "metadata": {},
   "source": [
    "现在，我们可以使用这个嵌入模型将文档存储到vectorstore中。为了简单起见，我们将使用一个简单的本地矢量库FAISS。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e3a009",
   "metadata": {},
   "source": [
    "首先，我们需要安装所需的软件包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f029399c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd727d0",
   "metadata": {},
   "source": [
    "然后我们可以建立索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0a853d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a39c2a",
   "metadata": {},
   "source": [
    "现在我们已经在vectorstore中为这些数据建立了索引，我们将创建一个检索链。该链将接收传入的问题，查找相关文档，然后将这些文档与原始问题一起传递给LLM，并要求它回答原始问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8671d8a9",
   "metadata": {},
   "source": [
    "首先，让我们建立一个链，它接受一个问题和检索到的文档，并生成一个答案。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95646a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4527d7",
   "metadata": {},
   "source": [
    "如果需要，我们可以通过直接传递文档来运行这个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08310ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "document_chain.invoke({\n",
    "    \"input\": \"how can langsmith help with testing?\",\n",
    "    \"context\": [Document(page_content=\"langsmith can let you visualize test results\")]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c78a1a1",
   "metadata": {},
   "source": [
    "然而，我们希望文档首先来自我们刚刚设置的检索器。这样，我们可以使用检索器动态地选择最相关的文档，并为给定的问题传递这些文档。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfcac3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba8e09a",
   "metadata": {},
   "source": [
    "现在我们可以调用这个链。这将返回一个字典，来自LLM的响应在答案键中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63014c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "print(response[\"answer\"])\n",
    "\n",
    "# LangSmith offers several features that can help with testing:..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5898753",
   "metadata": {},
   "source": [
    "这个答案应该准确得多"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497527d9",
   "metadata": {},
   "source": [
    "### Conversation Retrieval Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae62234d",
   "metadata": {},
   "source": [
    "我们目前创建的链只能回答单一的问题。人们正在构建的LLM应用程序的主要类型之一是聊天机器人。那么，我们如何把这条链变成一条能够回答后续问题的链呢?\n",
    "\n",
    "我们仍然可以使用create_retrieval_chain函数，但是我们需要改变两件事:\n",
    "\n",
    "1.检索方法现在不应该只处理最近的输入，而是应该考虑整个历史。\n",
    "\n",
    "2.最后的LLM链也应该考虑到整个历史"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923d6512",
   "metadata": {},
   "source": [
    "**Updating Retrieval**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee058132",
   "metadata": {},
   "source": [
    "为了更新检索，我们将创建一个新的链。该链将接受最近的输入(input)和对话历史(chat_history)，并使用LLM生成搜索查询。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e00754",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "# First we need a prompt that we can pass into an LLM to generate this search query\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"user\", \"Given the above conversation, generate a search query to look up to get information relevant to the conversation\")\n",
    "])\n",
    "retriever_chain = create_history_aware_retriever(llm, retriever, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaa8e6f",
   "metadata": {},
   "source": [
    "我们可以通过传递一个用户提出后续问题的实例来对此进行测试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce581261",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
    "retriever_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4842af1",
   "metadata": {},
   "source": [
    "您应该看到，这将返回关于在LangSmith中进行测试的文档。这是因为LLM生成了一个新的查询，将聊天历史记录与后续问题结合起来。\n",
    "\n",
    "现在我们有了这个新的检索器，我们可以创建一个新的链，以便在记住这些检索到的文档的情况下继续对话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75814f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the user's questions based on the below context:\\n\\n{context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fde8d5",
   "metadata": {},
   "source": [
    "我们现在可以端到端测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ccfeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
    "retrieval_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb2edef",
   "metadata": {},
   "source": [
    "我们可以看到，这给出了一个连贯的答案——我们已经成功地把检索链变成了一个聊天机器人!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ad2fdb",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30d59cb",
   "metadata": {},
   "source": [
    "We've so far created examples of chains - where each step is known ahead of time. The final thing we will create is an agent - where the LLM decides what steps to take."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd03f39",
   "metadata": {},
   "source": [
    "**NOTE: for this example we will only show how to create an agent using OpenAI models, as local models are not reliable enough yet.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51591334",
   "metadata": {},
   "source": [
    "One of the first things to do when building an agent is to decide what tools it should have access to. For this example, we will give the agent access to two tools:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7bcee7",
   "metadata": {},
   "source": [
    "1.The retriever we just created. This will let it easily answer questions about LangSmith\n",
    "\n",
    "2.A search tool. This will let it easily answer questions that require up-to-date information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47171a7",
   "metadata": {},
   "source": [
    "First, let's set up a tool for the retriever we just created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e36a523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"langsmith_search\",\n",
    "    \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289b07ed",
   "metadata": {},
   "source": [
    "The search tool that we will use is Tavily. This will require an API key (they have generous free tier). After creating it on their platform, you need to set it as an environment variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203a0071",
   "metadata": {},
   "outputs": [],
   "source": [
    "export TAVILY_API_KEY=..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e38e7a",
   "metadata": {},
   "source": [
    "If you do not want to set up an API key, you can skip creating this tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b86e397",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "search = TavilySearchResults()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33de6c4",
   "metadata": {},
   "source": [
    "We can now create a list of the tools we want to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0287c274",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [retriever_tool, search]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca0042a",
   "metadata": {},
   "source": [
    "Now that we have the tools, we can create an agent to use them. We will go over this pretty quickly - for a deeper dive into what exactly is going on, check out the Agent's Getting Started documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933bd2cc",
   "metadata": {},
   "source": [
    "Install langchain hub first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c94669",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchainhub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84256dce",
   "metadata": {},
   "source": [
    "Install the langchain-openai package To interact with OpenAI we need to use langchain-openai which connects with OpenAI SDK[https://github.com/langchain-ai/langchain/tree/master/libs/partners/openai]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4317cf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4709be",
   "metadata": {},
   "source": [
    "Now we can use it to get a predefined prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef28625",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "from langchain.agents import create_openai_functions_agent\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "\n",
    "# You need to set OPENAI_API_KEY environment variable or pass it as argument `openai_api_key`.\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "agent = create_openai_functions_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d101f1d5",
   "metadata": {},
   "source": [
    "We can now invoke the agent and see how it responds! We can ask it questions about LangSmith:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048a0df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e0a69e",
   "metadata": {},
   "source": [
    "We can ask it about the weather:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc7d6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": \"what is the weather in SF?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a93177f",
   "metadata": {},
   "source": [
    "We can have conversations with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876e56ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
    "agent_executor.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d20336c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
